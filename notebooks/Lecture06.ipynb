{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8c93ec",
   "metadata": {},
   "source": [
    "Today's topics:\n",
    "* Feature scaling\n",
    "* Reconstruction\n",
    "* Manifold learning\n",
    "* Semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ddfdf0",
   "metadata": {},
   "source": [
    "Let's load the alloys dataset from before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0443e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../datasets/steels.csv')  # load into pandas\n",
    "data                            # show a view of the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2ee0b",
   "metadata": {},
   "source": [
    "Drop the outlier we found last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = np.argmax(data[' Tensile Strength (MPa)'])\n",
    "clean_data = data.drop(index=outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cb6b9",
   "metadata": {},
   "source": [
    "And define a space to work in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46470b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_data.loc[:, ' 0.2% Proof Stress (MPa)':' Reduction in Area (%)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669a754",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e39177",
   "metadata": {},
   "source": [
    "One thing to watch out for when doing unsupervised representation learning is features with different units. If one of the features has magnitudes of $10^6$ and another has $10^1$, the big one will completely dominate the embedding. We can get around this using **feature scaling**.\n",
    "\n",
    "<img src=\"./assets/feature_scaling.jpg\" width=600 alt=\"Comparison of data distribution before and after different scaling methods: StandardScaler, MinMaxScaler, and RobustScaler\">\n",
    "\n",
    "Making sure the data are roughly isotropic really helps us capture more information in the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a82143",
   "metadata": {},
   "source": [
    "## Standard scaler\n",
    "\n",
    "Here we'll use the `StandardScaler`, which normalizes everything according to its standard deviation:\n",
    "\n",
    "$X_s = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "This way all the values should end up with comparable magnitudes. Let's see how it affects the manifold structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07541119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# compute the pca embedding\n",
    "pca = decomposition.PCA().fit(x)\n",
    "Po = pca.transform(x)\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Po[:, 0], Po[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f577af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# rescale the data\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "xs = scaler.transform(x)\n",
    "\n",
    "# compute the pca embedding\n",
    "pca = decomposition.PCA().fit(xs)\n",
    "P = pca.transform(xs)\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].scatter(P[:, 0], P[:, 1])\n",
    "ax[1].scatter(Po[:, 0], Po[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bdb03",
   "metadata": {},
   "source": [
    "We can try looking at this manifold through the lens of different compositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(P[:, 0], P[:, 1], c=clean_data[' Al'])\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(P[:, 0], P[:, 1], c=clean_data['V'])\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc276339",
   "metadata": {},
   "source": [
    "Remember - this was fitted to the properties! The fact that there are correlations with the compositions tells us something about how the chemistry influences properties -- for instance, we should look at V-containing steels to get properties like those on the left hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(x.columns, pca.components_[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x.columns, pca.components_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b3013",
   "metadata": {},
   "source": [
    "Just to show that feature scaling achieved something, this was the original projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA().fit(x)\n",
    "P = pca.transform(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(Po[:, 0], Po[:, 1], c=clean_data['V'])\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054554d",
   "metadata": {},
   "source": [
    "We can confirm that part of the space got smashed together along the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA().fit(x)\n",
    "Po = pca.transform(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x.columns, pca.components_[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x.columns, pca.components_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26aa8b",
   "metadata": {},
   "source": [
    "## Min-Max scaler\n",
    "\n",
    "This is the most straightforward preprocesser you could imagine -- it simply rescales each column to have a min value of 0 and max value of 1.\n",
    "\n",
    "$x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551219dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler().fit(x)\n",
    "xs = scaler.transform(x)\n",
    "\n",
    "pca = decomposition.PCA().fit(xs)\n",
    "P = pca.transform(xs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(P[:, 0], P[:, 1], c=clean_data['V'])\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c829ca9",
   "metadata": {},
   "source": [
    "One interesting consequence of the `MinMaxScaler` is the mitigation of the outlier in `Tensile Strength (MPa)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596333b",
   "metadata": {},
   "source": [
    "## Power transformer\n",
    "\n",
    "The power transformer is a scheme to make a non-Gaussian distribution look more Gaussian.\n",
    "Here's a visual example:\n",
    "\n",
    "<img src=\"./assets/power_transformer.jpg\" width=600 alt=\"Effect of PowerTransformer on skewed data distributions to make them more Gaussian-like\">\n",
    "\n",
    "Why would we do this?\n",
    "This is an attempt to avoid the long tails dominating the reduced space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.PowerTransformer().fit(x)\n",
    "xs = scaler.transform(x)\n",
    "\n",
    "pca = decomposition.PCA().fit(xs)\n",
    "P = pca.transform(xs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(P[:, 0], P[:, 1], c=clean_data['V'])\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0571005",
   "metadata": {},
   "source": [
    "Here the power transformer provides a middle ground between the standard scaler and min-max scaler.\n",
    "Note that all three of these preprocessing methods make the point cloud more isotropic and portray additional features compared to the PCA on raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad61feb",
   "metadata": {},
   "source": [
    "A quick note on the inverse transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'original', x.values[0] )\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "xs = scaler.transform(x)\n",
    "print( 'scaled', xs[0] )\n",
    "print( 'reconstructed', scaler.inverse_transform(xs)[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a80f26",
   "metadata": {},
   "source": [
    "## [Check your understanding]\n",
    "\n",
    "Try implementing the `StandardScaler` and `MinMaxScaler` yourself (i.e., using `numpy`).\n",
    "\n",
    "Check that you have it right by comparing to the result from the `sklearn` builtins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = scaler.transform(x)\n",
    "print( 'scaled', xs[0] )\n",
    "\n",
    "mu = x.mean(axis=0)\n",
    "sigma = x.std(axis=0)\n",
    "xs = (x - mu) / sigma\n",
    "print(xs.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7c1b9",
   "metadata": {},
   "source": [
    "# Reconstruction\n",
    "\n",
    "Something we haven't discussed with these dimensionality reduction approaches is the concept of reconstructing the high-dimensional object from the low-dimensional embedding.\n",
    "It's important to understand how this works, its limitations, and how it might be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee459fd",
   "metadata": {},
   "source": [
    "## Quick review of PCA\n",
    "\n",
    "Let's review how the PCA projection works.\n",
    "We'll go back to the alloy compositions data for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438edf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "x = data.loc[:, ' C':'Nb + Ta']\n",
    "\n",
    "pca = decomposition.PCA().fit(x)\n",
    "z = pca.transform(x)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773a8f9",
   "metadata": {},
   "source": [
    "Now that we have this $z$ embedding, we can evaluate the data in this low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b24ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*z[:, :2].T)  # shorthand way to plot the columns (1, 2) as (x, y)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a913e10",
   "metadata": {},
   "source": [
    "Remember these projections are nothing more than the product of the original array (shifted to have zero mean) with the principal component vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37696982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use x.values because x is actually a DataFrame! .values converts to array\n",
    "x_shift = x.values - np.mean(x.values, axis=0)\n",
    "# we use the transpose of pca.components_ based on the implementation in sklearn\n",
    "z_manual = np.dot(x_shift, pca.components_.T)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*z_manual[:, :2].T)  # shorthand way to plot the columns (1, 2) as (x, y)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6e878",
   "metadata": {},
   "source": [
    "## Concept of reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e44cf3",
   "metadata": {},
   "source": [
    "We can also take these embeddings and \"un-project\" them back to the originals.\n",
    "(Remember this embedding operation was basically a rotation).\n",
    "Let's evaluate these in the 2D plane that corresponds to the greatest variance in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascending = np.argsort(pca.components_[0])\n",
    "descending = ascending[::-1]\n",
    "top2 = descending[:2]\n",
    "print(top2)\n",
    "print(x.columns[top2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recon = np.dot(z_manual, pca.components_) + np.mean(x.values, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*x_recon[:, top2].T, label='Reconstruction')\n",
    "ax.scatter(*x.values[:, top2].T, label='Original',\n",
    "           marker='s', edgecolor='tab:orange', facecolor='none')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1efdd3",
   "metadata": {},
   "source": [
    "## Information loss\n",
    "\n",
    "We can plot the cumulative explained variance to see how many of these are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n = np.arange(pca.n_components_)+1\n",
    "var_c = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(var_c)\n",
    "ax.plot(n, var_c, '.-')\n",
    "ax.set_xlabel('Components')\n",
    "ax.set_ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71148e",
   "metadata": {},
   "source": [
    "While this is slightly subjective (or at least depends on the intended application), we can probably say that we won't be able to see a difference with more than 9 components (>99.99% explained variance).\n",
    "Let's investigate what happens when we use less or more components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 9\n",
    "z_trunc = z_manual[:, :nc]\n",
    "x_recon = np.dot(z_trunc, pca.components_[:nc, :]) + np.mean(x.values, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*x_recon[:, top2].T, label='Reconstruction')\n",
    "ax.scatter(*x.values[:, top2].T, label='Original',\n",
    "           marker='s', edgecolor='tab:orange', facecolor='none')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b35da",
   "metadata": {},
   "source": [
    "So far, so good. Let's make a function and try using subsequently fewer components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(nc):\n",
    "    z_trunc = z_manual[:, :nc]\n",
    "    x_recon = np.dot(z_trunc, pca.components_[:nc, :]) + np.mean(x.values, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(*x_recon[:, top2].T, label='Reconstruction')\n",
    "    ax.scatter(*x.values[:, top2].T, label='Original',\n",
    "            marker='s', edgecolor='tab:orange', facecolor='none')\n",
    "    ax.legend()\n",
    "    ax.text(0.95, 0.05, f'{nc} components; {var_c[nc]*100:.2f}% explained variance',\n",
    "            ha='right', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_xlabel(x.columns[top2[0]])\n",
    "    ax.set_ylabel(x.columns[top2[1]])\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_reconstruction(8)\n",
    "fig = plot_reconstruction(6)\n",
    "fig = plot_reconstruction(4)\n",
    "fig = plot_reconstruction(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c2c7b",
   "metadata": {},
   "source": [
    "Here we see that the fidelity of the reconstruction decays rapidly as we move to fewer components.\n",
    "However, these were the dominant elements in the original space.\n",
    "If we consider some other elements we'll see much worse reconstruction much earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26277d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 8  # looked fine for [Cr, Mo]\n",
    "\n",
    "z_trunc = z_manual[:, :nc]\n",
    "x_recon = np.dot(z_trunc, pca.components_[:nc, :]) + np.mean(x.values, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*x_recon[:, :2].T, label='Reconstruction')\n",
    "ax.scatter(*x.values[:, :2].T, label='Original',\n",
    "        marker='s', edgecolor='tab:orange', facecolor='none')\n",
    "ax.legend()\n",
    "ax.text(0.95, 0.05, f'{nc} components; {var_c[nc]*100:.2f}% explained variance',\n",
    "        ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(x.columns[0])\n",
    "ax.set_ylabel(x.columns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59487bc7",
   "metadata": {},
   "source": [
    "Since the principal components did not consider the variations in these (Si, C), they are not captured perfectly here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec269ab4",
   "metadata": {},
   "source": [
    "## [Check your understanding]\n",
    "\n",
    "Try repeating this reconstruction experiment using rescaled features.\n",
    "Follow these steps:\n",
    "* Apply a feature scaler to the data\n",
    "* Plot the PCA embedding from scaled features\n",
    "* Plot the explained variance curve\n",
    "* Try reconstructing the original data from the low-dimensional embeddings\n",
    "> Note: you will have to invert the feature scaling in addition to the embedding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3fc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1df1587",
   "metadata": {},
   "source": [
    "# Manifold learning\n",
    "\n",
    "Manifold learning is another term for nonlinear dimensionality reduction.\n",
    "Roughly speaking, a [manifold](https://en.wikipedia.org/wiki/Manifold) is an $n$-dimensional surface that is locally smooth.\n",
    "The term \"manifold learning\" therefore means searching for a smooth, low-dimensional surface in a high-dimensional space.\n",
    "\n",
    "Here's a nice motivating example that shows the difference between a **linear projection** and a **nonlinear manifold**:\n",
    "\n",
    "<img src=\"./assets/manifold_learning_spiral.jpg\" height=400 alt=\"A 3D spiral dataset representing a manifold that can be unrolled into 2D\">\n",
    "\n",
    "If we plot the data just based on the $x$ coordinate, we end up with the picture on the left. But if we find patterns in the data using unsupervised learning, we can \"unroll\" the spiral and get a new representation like the one on the right.\n",
    "\n",
    "Let's try this with a toy dataset before we move to our alloy compositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from plotly import express as px\n",
    "\n",
    "S, t = datasets.make_swiss_roll(n_samples=400)\n",
    "\n",
    "px.scatter_3d(x=S[:, 0], y=S[:, 1], z=S[:, 2], color=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468138b",
   "metadata": {},
   "source": [
    "If we try to apply PCA on this data, we will not get what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# fit the model\n",
    "pca = decomposition.PCA().fit(S)\n",
    "St = pca.transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(St[:, 0], St[:, 1], c=t)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cc1d5",
   "metadata": {},
   "source": [
    "Why?\n",
    "Because PCA is a linear projection and linear methods can never reproduce nonlinear behavior.\n",
    "We need to introduce a nonlinearity in our learning pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff9b68",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "We can apply the \"kernel trick\" to use PCA for nonlinear data.\n",
    "First we apply the nonlinear kernel and then we do the linear projection.\n",
    "This is very similar to the example of polynomial features in a linear regression.\n",
    "\n",
    "Here is a great visual example of the kernel trick at work:\n",
    "\n",
    "<img src=\"./assets/kernel_trick.jpg\" width=600 alt=\"Diagram illustrating the kernel trick: projecting linearly inseparable data into a higher dimension where it becomes separable\">\n",
    "\n",
    "After this nonlinear transformation, the decision boundary can be very easily defined by a linear model.\n",
    "\n",
    "Let's try the `poly` kernel on the nonlinear data above.\n",
    "Note that for manifold learning, `sklearn` uses `fit()` and `transform()` just like the PCA interface instead of the `predict()` for supervised learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "pca = decomposition.KernelPCA(kernel='poly').fit(S)\n",
    "\n",
    "# project X using PCA\n",
    "St = pca.transform(S)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(St[:, 0], St[:, 1], c=t)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bf1d7",
   "metadata": {},
   "source": [
    "Unfortunately, this result doesn't look any \"better\" than the linear PCA.\n",
    "We might also try the `rbf` option, which stands for Radial Basis Function.\n",
    "This is a localized basis function that links nearby points together, which makes sense when trying to identify this spiral manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "pca = decomposition.KernelPCA(kernel='rbf').fit(S)\n",
    "\n",
    "# project X using PCA\n",
    "St = pca.transform(S)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(St[:, 0], St[:, 1], c=t)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ffc4f",
   "metadata": {},
   "source": [
    "Nope, the RBF detects the wrong structure in the data.\n",
    "That's fine, there's no guarantee that more complex models will yield better results for any given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a07300",
   "metadata": {},
   "source": [
    "## Spectral methods\n",
    "\n",
    "The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:\n",
    "\n",
    "**Weighted Graph Construction.**\n",
    "Transform the raw input data into graph representation using affinity (adjacency) matrix representation.\n",
    "\n",
    "**Graph Laplacian Construction.**\n",
    "The unnormalized Graph Laplacian is constructed as $L = D - A$ and normalized as\n",
    "\n",
    "$L = D^{-1/2} (D-A) D^{-1/2}$\n",
    "\n",
    "**Partial Eigenvalue Decomposition.**\n",
    "Eigenvalue decomposition is done on graph Laplacian\n",
    "\n",
    "This is very similar to the idea of PCA, except we will use a nonlinear, graph-based construction for $A$ instead of the covariance matrix.\n",
    "After constructing the matrix, the spectral decomposition (eigenvalue problem) is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e8840",
   "metadata": {},
   "source": [
    "One of the main decisions here is what constitutes adjacency.\n",
    "The default in `sklearn` is to build the nearest neighbors graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "Z = manifold.SpectralEmbedding().fit_transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=t)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf79a4",
   "metadata": {},
   "source": [
    "The default for `affinity` is the nearest neighbor graph, which does not appear to work well.\n",
    "What if we try the `rbf` option like we did above for the kernel PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c974187",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = manifold.SpectralEmbedding(affinity='rbf').fit_transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=t)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c7b4f",
   "metadata": {},
   "source": [
    "This actually works pretty well!\n",
    "Note that the curvature in $Z_1$ is entirely spurious and the discovered manifold is entirely 1D.\n",
    "This is a common feature of manifold learning approaches when there are extra dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a9e4e",
   "metadata": {},
   "source": [
    "## Diffusion maps\n",
    "\n",
    "A variant of the spectral embedding is to use a Gaussian kernel to emulate a \"diffusion process\" on the manifold:\n",
    "\n",
    "$k(x,y) = \\exp \\left( -\\frac{||x-y||^2}{\\epsilon^2} \\right)$\n",
    "\n",
    "With this kernel we can compute an affinity matrix $L$ and then solve the eigenvector problem on the normalized diffusion matrix,\n",
    "\n",
    "$P = D^{-1} K$,\n",
    "\n",
    "this can be cast as an eigenvector problem to obtain a mapping in the diffusion space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "dist = distance.squareform(distance.pdist(S))\n",
    "\n",
    "epsilon = np.percentile(dist, 1)\n",
    "L = np.exp(-dist**2/epsilon**2)\n",
    "\n",
    "D = np.diag(np.sum(L, axis=1))\n",
    "P = np.linalg.inv(D) @ L\n",
    "\n",
    "w, v = np.linalg.eig(P)\n",
    "plt.scatter(*v[:, 1:3].T, c=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde77da",
   "metadata": {},
   "source": [
    "## Other spectral methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23770b5",
   "metadata": {},
   "source": [
    "Here are a few other methods from the `manifold` module in no particular order.\n",
    "You can read about their assumptions in the `scikit-learn` documentation or on other websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "Z = manifold.Isomap().fit_transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=t)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = manifold.LocallyLinearEmbedding().fit_transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=t)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = manifold.MDS().fit_transform(S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=t)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fe534",
   "metadata": {},
   "source": [
    "## Where manifold learning fails\n",
    "\n",
    "Let's revisit our composition data from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e14243",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.loc[:, ' C':'Nb + Ta']\n",
    "# here's a one-liner to encode the str labels as int:\n",
    "_, y = np.unique([it[0] for it in data['Alloy code']], return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e757a",
   "metadata": {},
   "source": [
    "We can try `SpectralEmbedding` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = manifold.SpectralEmbedding(random_state=0).fit_transform(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=y)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb649107",
   "metadata": {},
   "source": [
    "The `nearest_neighbors` affinity does not seem to be working.\n",
    "Let's go back to `rbf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = manifold.SpectralEmbedding(affinity='rbf', random_state=0).fit_transform(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=y)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aefbf1",
   "metadata": {},
   "source": [
    "This is almost identical to PCA.\n",
    "What about diffusion maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = distance.squareform(distance.pdist(x))\n",
    "\n",
    "epsilon = np.percentile(dist, 100)\n",
    "L = np.exp(-dist**2/epsilon**2)\n",
    "\n",
    "D = np.diag(np.sum(L, axis=1))\n",
    "P = np.linalg.inv(D) @ L\n",
    "\n",
    "w, v = np.linalg.eig(P)\n",
    "plt.scatter(*v[:, 1:3].T, c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb3aa2",
   "metadata": {},
   "source": [
    "You can see that depending on the choice of $\\varepsilon$ we will get wildly different results, eventually ending up back at something like the linear result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b2878",
   "metadata": {},
   "source": [
    "# Semi-supervised learning\n",
    "\n",
    "As a final comment on unsupervised learning, we should note that it is possible to come across problems that are best solved with hybrid unsupervised + supervised learning schemes.\n",
    "In these cases, we might have a few labeled points (e.g., from a very expensive experiment or simulation) while the vast majority are unlabeled.\n",
    "More than either supervised or unsupervised, this can require a lot of expert knowledge to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df7141",
   "metadata": {},
   "source": [
    "## In `scikit-learn`\n",
    "\n",
    "Let's try out one of the `sklearn.semi_supervised` builtins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81191006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import semi_supervised\n",
    "\n",
    "x = data.loc[:, ' C':'Nb + Ta']\n",
    "p = decomposition.PCA().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126152ad",
   "metadata": {},
   "source": [
    "Let's remove 99% of the data by replacing the class labels with `-1` (per the instructions in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "remove_n = int(0.99 * x.shape[0])  # remove 98% of the labels!\n",
    "\n",
    "sparse_y = np.array(y)  # create a copy of the labels\n",
    "\n",
    "rng = np.random.RandomState(0)  # set random state so we always get same result\n",
    "remove_idx = rng.choice(np.arange(y.shape[0]), remove_n, replace=False)\n",
    "\n",
    "sparse_y[remove_idx] = -1  # remove some labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425731bb",
   "metadata": {},
   "source": [
    "We can visualize the result of this \"un-labeling\" by splitting the dataset up in the scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sparse_y > 0  # separate the plotted points into labeled/unlabeled\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(p[idx, 0], p[idx, 1], c=sparse_y[idx])\n",
    "ax.plot(p[~idx, 0], p[~idx, 1], '.', color=np.ones(3)*0.8, zorder=0)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d70d47",
   "metadata": {},
   "source": [
    "Now we fit the model and check its performance against the full labeled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = semi_supervised.LabelSpreading().fit(p, sparse_y)\n",
    "print(model.score(p, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba4ffa",
   "metadata": {},
   "source": [
    "This performance may surprise you.\n",
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = model.predict(p)\n",
    "incorrect = (predicted_class^y).astype(bool)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(p[:, 0], p[:, 1], c=predicted_class)\n",
    "ax.plot(p[incorrect, 0], p[incorrect, 1], 'rx')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061a252",
   "metadata": {},
   "source": [
    "## Unsupervised UMAP\n",
    "\n",
    "Uniform Manifold APproximation uses more sophisticated machinery to compute something like the spectral embedding.\n",
    "It ends up being able to resolve irregularly space data like [this visualization of UMAP topology](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html):\n",
    "\n",
    "<img src=\"./assets/umap_graph.jpg\" width=600 alt=\"Visualization of a high-dimensional dataset projected into 2D using the UMAP manifold learning algorithm\">\n",
    "\n",
    "To use the package, we first need to install the UMAP package using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c897a8",
   "metadata": {},
   "source": [
    "Now we can import the `umap` module and fit a `UMAP` object.\n",
    "It has an interface just like `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "Z = umap.UMAP(random_state=0).fit_transform(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=y)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4297b",
   "metadata": {},
   "source": [
    "We can see here the result looks quite different from PCA -- instead of a few contiguous clusters there are many discrete clusters spread across the space.\n",
    "We can investigate this result using `plotly.express`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f559a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=Z[:, 0], y=Z[:, 1], color=y, hover_name=data['Alloy code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341edfc7",
   "metadata": {},
   "source": [
    "## Supervised UMAP\n",
    "\n",
    "We can also do supervised and semi-supervised learning with UMAP, to create clusters that are informed by the labels.\n",
    "Here's the fully supervised case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = umap.UMAP(random_state=0).fit_transform(x, y=y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=y)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a409d55",
   "metadata": {},
   "source": [
    "In this particular case, it doesn't change much (if at all).\n",
    "For completeness, here's the semi-supervised case, where we artificially hide some of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_y = np.array(y)  # create a copy of the labels\n",
    "\n",
    "remove_n = int(0.90 * x.shape[0])  # remove 90% of the labels!\n",
    "\n",
    "rng = np.random.RandomState(0)  # set random state so we always get same result\n",
    "remove_idx = rng.choice(np.arange(y.shape[0]), remove_n, replace=False)\n",
    "\n",
    "sparse_y[remove_idx] = -1  # this indicates \"missing\" label\n",
    "\n",
    "Z = umap.UMAP(random_state=0).fit_transform(x, y=sparse_y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=y)\n",
    "ax.set_xlabel('$Z_0$')\n",
    "ax.set_ylabel('$Z_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01729faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-id,-colab,-outputId",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
