{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ae09ff",
   "metadata": {},
   "source": [
    "Today's topics:\n",
    "* Bayesian optimization for hyperparameter tuning\n",
    "* Evolutionary algorithm for feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ca002",
   "metadata": {},
   "source": [
    "Start by loading the alloys mechanical properties dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65640e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../datasets/steels.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718d81d",
   "metadata": {},
   "source": [
    "Select the features and regression labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.loc[:, ' C':' Temperature (Â°C)']\n",
    "y = data[' Tensile Strength (MPa)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050dea77",
   "metadata": {},
   "source": [
    "Let's train a multivariate linear regression as a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "folds = model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "results = []\n",
    "for train_index, test_index in folds.split(x):\n",
    "    # split the data\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    # train a model\n",
    "    model.fit(x_train, y_train)\n",
    "    # evaluate r2 on validation set\n",
    "    r2 = model.score(x_val, y_val)\n",
    "    results.append(r2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a808724f",
   "metadata": {},
   "source": [
    "And if we try a tree-based scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "\n",
    "model = ensemble.RandomForestRegressor(random_state=0)\n",
    "\n",
    "folds = model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "results = []\n",
    "for train_index, test_index in folds.split(x):\n",
    "    # split the data\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    # train a model\n",
    "    model.fit(x_train, y_train)\n",
    "    # evaluate r2 on validation set\n",
    "    r2 = model.score(x_val, y_val)\n",
    "    results.append(r2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99870ebf",
   "metadata": {},
   "source": [
    "Random Forest is much better!\n",
    "Although there is still one fold that gives poor validation performance.\n",
    "What about a Neural Network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import neural_network\n",
    "\n",
    "model = neural_network.MLPRegressor(random_state=0)\n",
    "\n",
    "folds = model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "results = []\n",
    "for train_index, test_index in folds.split(x):\n",
    "    # split the data\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    # train a model\n",
    "    model.fit(x_train, y_train)\n",
    "    # evaluate r2 on validation set\n",
    "    r2 = model.score(x_val, y_val)\n",
    "    results.append(r2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1fc4d",
   "metadata": {},
   "source": [
    "I mentioned before that neural networks require more extensive hyperparameter tuning.\n",
    "Let's explore methods for doing that now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc51f6",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "<img src=\"./assets/bayesian_optimization_workflow.jpg\" width=600 alt=\"Flowchart of the Bayesian Optimization process: surrogate model, acquisition function, and objective evaluation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacf448",
   "metadata": {},
   "source": [
    "## Gaussian process\n",
    "\n",
    "<img src=\"./assets/gp_noise_gp.jpg\" width=600 alt=\"Visualization of a Gaussian Process regression with confidence intervals and noisy data points\">\n",
    "\n",
    "<img src=\"./assets/kernel_types.jpg\" width=600 alt=\"Visual gallery of different Gaussian Process kernels and their resulting functions\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f48e7",
   "metadata": {},
   "source": [
    "## Basic hyperparameter tuning\n",
    "\n",
    "We will use the `ax-platform` package for hyperparameter tuning.\n",
    "It is more convenient than implementing this ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae53bce",
   "metadata": {},
   "source": [
    "Now we need to do a train/validation/test split and an objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8080fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trv, x_test, y_trv, y_test = model_selection.train_test_split(x, y, test_size=0.20, shuffle=True, random_state=0)\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(x_trv, y_trv, train_size=0.75, shuffle=True, random_state=0)\n",
    "\n",
    "def mlp_fitness(parameterization):\n",
    "    try:\n",
    "        model = neural_network.MLPRegressor(**parameterization, random_state=0).fit(x_train, y_train)\n",
    "        score = model.score(x_val, y_val)\n",
    "    except:\n",
    "        score = -1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057eb9b",
   "metadata": {},
   "source": [
    "Now we set up the optimization problem in the format specified by `ax-platform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae61986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.managed_loop import optimize\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"learning_rate_init\", \"type\": \"range\", \"bounds\": [1e-6, 1e-1], \"log_scale\": True},\n",
    "        {\"name\": \"max_iter\", \"type\": \"range\", \"bounds\": [10, 1000]},\n",
    "    ],\n",
    "    evaluation_function=mlp_fitness,\n",
    "    objective_name='r-squared',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d162ee",
   "metadata": {},
   "source": [
    "With Gaussian Process, we can evaluate the results in terms of both mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( best_parameters )\n",
    "means, covariances = values\n",
    "print( means, covariances )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2982",
   "metadata": {},
   "source": [
    "It is very helpful to visualize the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f16970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.plot.contour import plot_contour\n",
    "from ax.utils.notebook.plotting import render\n",
    "\n",
    "render(plot_contour(model=model, param_x='learning_rate_init', param_y='max_iter', metric_name='r-squared'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49482d47",
   "metadata": {},
   "source": [
    "And we can also view it as a function of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf57f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "import numpy as np\n",
    "\n",
    "best_objectives = np.array([[trial.objective_mean for trial in experiment.trials.values()]])\n",
    "results = np.maximum.accumulate(best_objectives, axis=1).tolist()\n",
    "px.line(y=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234f27e",
   "metadata": {},
   "source": [
    "Now that we have the optimal result, we should train a model with those hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae13047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPRegressor(**best_parameters, random_state=0).fit(x_train, y_train)\n",
    "print( 'r-squared, val: ', model.score(x_val, y_val))\n",
    "print( 'r-squared, test:', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22921ed2",
   "metadata": {},
   "source": [
    "## Categorical hyperparameters\n",
    "\n",
    "We can also specify discrete options such as the activation functions.\n",
    "These can have a huge effect on the results:\n",
    "\n",
    "<img src=\"./assets/activation_functions.jpg\" width=600 alt=\"Plots of common neural network activation functions including Sigmoid, Tanh, and ReLU\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fff229",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"learning_rate_init\", \"type\": \"range\", \"bounds\": [1e-6, 1e-1], \"log_scale\": True},\n",
    "        {\"name\": \"max_iter\", \"type\": \"range\", \"bounds\": [10, 1000]},\n",
    "        {\"name\": \"activation\", \"type\": \"choice\", \"values\": [\"identity\", \"logistic\", \"tanh\", \"relu\"]}\n",
    "    ],\n",
    "    evaluation_function=mlp_fitness,\n",
    "    objective_name='r-squared',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f41304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41c49f",
   "metadata": {},
   "source": [
    "## [Check your understanding]\n",
    "\n",
    "Use `ax-platform` to optimize the `solver`.\n",
    "Refer to the documentation for the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b49f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd030d5",
   "metadata": {},
   "source": [
    "## Architecture optimization\n",
    "\n",
    "Deep learning models may have very complex architecture with many hyperparameters to choose:\n",
    "\n",
    "<img src=\"./assets/neural_network_architecture.jpg\" width=600 alt=\"Schematic diagram of a multi-layer perceptron neural network with input, hidden, and output layers\">\n",
    "\n",
    "In this case, we need to be clever in how to encode the many possible options.\n",
    "Here are some common shapes for NNs:\n",
    "\n",
    "<img src=\"./assets/neural_network_zoo.jpg\" width=400 alt=\"Graphic showing various neural network architecture types beyond simple feed-forward networks\">\n",
    "\n",
    "You will see that NNs do not typically have wildly oscillating sizes between layers.\n",
    "Instead, they vary smoothly and the typical shapes are flat or trapezoidal.\n",
    "This means we can reduce the number of parameters from choosing every number of neurons independently to only choosing the \"shape\" of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf664b",
   "metadata": {},
   "source": [
    "## [Check your understanding]\n",
    "\n",
    "First, write a function that gives you a list `hidden_layer_sizes` that encodes the number of neurons in each layer from a simpler parameterization (e.g., number of layers `n`, number of neurons in the first layer `n_init`, and number of neurons in the last layer, `n_last`).\n",
    "\n",
    "Now use `ax-platform` to optimize the `hidden_layer_sizes` using your parameterization above.\n",
    "\n",
    "Refer to the `MLPRegressor` documentation for syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9dad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fb5e29",
   "metadata": {},
   "source": [
    "# Evolutionary algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704aba36",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "\"Evolutionary algorithm,\" \"genetic algorithm,\" or \"evolutionary optimization\" is a scheme that utilizes the idea of natural selection to perform numerical optimization:\n",
    "\n",
    "<img src=\"./assets/genetic_algorithm_concept.jpg\" width=600 alt=\"Visual metaphor for Genetic Algorithms based on biological evolution: selection, crossover, and mutation\">\n",
    "\n",
    "In each \"generation,\" traits from the best individuals are combined in a process analagous to gene transfer between DNA of parents in biological organisms:\n",
    "\n",
    "<img src=\"./assets/genetic_algorithm_flow.jpg\" width=600 alt=\"Step-by-step flowchart of the Genetic Algorithm iterative loop\">\n",
    "\n",
    "We also include mutations to permit new traits to arise in the population:\n",
    "\n",
    "<img src=\"./assets/genetic_algorithm_operators.jpg\" width=600 alt=\"Visual detail of Genetic Algorithm operators: Crossover (recombination) and Mutation\">\n",
    "\n",
    "If the new traits lead to greater fitness, they persist and are passed on to future generations.\n",
    "\n",
    "Why use evolutionary algorithm (EA) over Gaussian Process (GP)?\n",
    "One answer is that GP stops working well in higher dimensions due to the ambiguity of distances in those high-dimensional spaces.\n",
    "Another is that GP is meant for continuous spaces, while we often have discrete choices in model tuning.\n",
    "EA has no problem with high-dimensional spaces as the crossover and mutation can occur in any dimension.\n",
    "In addition, GP scales like $\\mathcal{O}(N^3)$, which can get out of hand quickly.\n",
    "EA has no fitting so it's simply $\\mathcal{O}(N)$ -- although it may converge less quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15530f34",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Let's consider an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# set up train/validation/test sets\n",
    "x_trv, x_test, y_trv, y_test = model_selection.train_test_split(x, y, test_size=0.20, shuffle=True, random_state=0)\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(x_trv, y_trv, train_size=0.75, shuffle=True, random_state=0)\n",
    "\n",
    "# set up fitness function\n",
    "def fitness(features, solution_index):\n",
    "    \"\"\"A fitness function that selects features based on the input array and trains a KNeighborsRegressor.\n",
    "    It returns the model R-squared on validation data.\"\"\"\n",
    "    f = np.argwhere(features > 0.5).flatten()\n",
    "    model = neighbors.KNeighborsRegressor().fit(x_train.iloc[:, f], y_train)\n",
    "    r_squared = model.score(x_val.iloc[:, f], y_val)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4ab2b",
   "metadata": {},
   "source": [
    "Let's make sure this does what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.ones(x.shape[1])\n",
    "print( fitness(all_features, None) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860abb11",
   "metadata": {},
   "source": [
    "Compared to the full feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ace8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor().fit(x_train, y_train)\n",
    "print('validation: ', model.score(x_val, y_val))\n",
    "print('test: ', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb6b94",
   "metadata": {},
   "source": [
    "Set up the `pygad` optimization problem (it has a lot of parameters!!!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "\n",
    "features = np.zeros(x.shape[1], dtype=int)\n",
    "fitness_function = fitness\n",
    "\n",
    "num_generations = 16\n",
    "num_parents_mating = 2\n",
    "\n",
    "sol_per_pop = 8\n",
    "num_genes = len(features)\n",
    "\n",
    "parent_selection_type = \"sss\"\n",
    "keep_parents = 1\n",
    "\n",
    "crossover_type = \"single_point\"\n",
    "\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 10\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       fitness_func=fitness_function,\n",
    "                       sol_per_pop=sol_per_pop,\n",
    "                       num_genes=num_genes,\n",
    "                       init_range_low=0,\n",
    "                       init_range_high=1,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       random_seed=0, save_solutions=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5dc0a",
   "metadata": {},
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc49ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efd016",
   "metadata": {},
   "source": [
    "View the best solution and its fitness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16facae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Parameters of the best solution : \", x.columns[solution>0.5])\n",
    "print(\"Fitness value of the best solution = \", solution_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa04dd3",
   "metadata": {},
   "source": [
    "We can plot the result as a function of iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ga_instance.plot_fitness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f2962",
   "metadata": {},
   "source": [
    "We can visualize which features were selected by looking back at the `solutions` attribute of the `ga_instance`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "solutions = np.array(ga_instance.solutions) > 0.5\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 6), sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "_ = ax.imshow(np.array(ga_instance.solutions_fitness).reshape(1, -1))\n",
    "\n",
    "ax = axes[1]\n",
    "_ = ax.set_yticks(np.arange(x.shape[1]))\n",
    "_ = ax.set_yticklabels(x.columns)\n",
    "_ = ax.imshow(solutions.T)\n",
    "_ = ax.set_xlabel('Solution index')\n",
    "_ = ax.set_ylabel('Feature index')\n",
    "_ = ax.set_aspect('auto')\n",
    "\n",
    "plt.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5bc94",
   "metadata": {},
   "source": [
    "And like with `ax-platform`, we can also check test performance in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.argwhere(solution > 0.5).flatten()\n",
    "model = neighbors.KNeighborsRegressor().fit(x_train.iloc[:, f], y_train)\n",
    "print('validation: ', model.score(x_val.iloc[:, f], y_val))\n",
    "print('test: ', model.score(x_test.iloc[:, f], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d74a16",
   "metadata": {},
   "source": [
    "We got lucky here in that our test set actually performs better than our validation set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9022c01",
   "metadata": {},
   "source": [
    "Let's also quickly compare to what we would get from recursive drop-column feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column_importance(model, xtrain, ytrain, xtest, ytest):\n",
    "    \"\"\"Compute the drop-column importance on a trained model.\"\"\"\n",
    "\n",
    "    model.fit(xtrain, ytrain)\n",
    "    baseline = model.score(xtest, ytest)\n",
    "\n",
    "    dropped = np.zeros_like(xtest.columns)\n",
    "    for i, col in enumerate(xtest.columns):\n",
    "        x_dropped_train = xtrain.copy().drop(columns=col)\n",
    "        x_dropped_test = xtest.copy().drop(columns=col)\n",
    "        model.fit(x_dropped_train, ytrain)\n",
    "        dropped[i] = model.score(x_dropped_test, ytest)\n",
    "\n",
    "    return baseline, dropped\n",
    "\n",
    "\n",
    "def choose_worst_feature(model, x_train, y_train, x_val, y_val):\n",
    "    baseline, dropped = drop_column_importance(model, x_train, y_train, x_val, y_val)\n",
    "    return baseline, dropped, np.argmax(dropped)\n",
    "\n",
    "\n",
    "model = neighbors.KNeighborsRegressor()\n",
    "\n",
    "x_train_trim = x_train.copy()\n",
    "x_val_trim = x_val.copy()\n",
    "\n",
    "r2 = []\n",
    "feature_order = []\n",
    "\n",
    "for k in range(x_train_trim.shape[1]-1):\n",
    "    baseline, dropped, worst = choose_worst_feature(model, x_train_trim, y_train, x_val_trim, y_val)\n",
    "    r2.append(baseline)\n",
    "    feature_order.append(x_train_trim.columns[worst])\n",
    "    x_train_trim = x_train_trim.drop(columns=[x_train_trim.columns[worst]])\n",
    "    x_val_trim = x_val_trim.drop(columns=[x_val_trim.columns[worst]])\n",
    "\n",
    "# we have one feature left that we didn't drop:\n",
    "last_feature = x_train_trim.columns[0]\n",
    "feature_order.append( last_feature )\n",
    "\n",
    "print(np.round(r2, 2))\n",
    "print(feature_order)  # later is better (dropped last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd2863",
   "metadata": {},
   "source": [
    "If we compare that to the results from EA we may see something interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.argwhere(solution > 0.5).flatten()\n",
    "\n",
    "print( x.columns[f] )\n",
    "\n",
    "model = neighbors.KNeighborsRegressor().fit(x_train.iloc[:, f], y_train)\n",
    "print('validation: ', model.score(x_val.iloc[:, f], y_val))\n",
    "print('test: ', model.score(x_test.iloc[:, f], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc31456",
   "metadata": {},
   "source": [
    "The features are different -- how do they compare in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_rfe = feature_order[-len(f):]\n",
    "print(top_features_rfe)\n",
    "\n",
    "model = model.fit(x_train.loc[:, top_features_rfe], y_train)\n",
    "\n",
    "print( 'validation: ', model.score(x_val.loc[:, top_features_rfe], y_val) )\n",
    "print( 'test: ', model.score(x_test.loc[:, top_features_rfe], y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8402dfa",
   "metadata": {},
   "source": [
    "## Interaction between feature selection and hyperparameter tuning\n",
    "\n",
    "This feature selection scheme is only half of a complete workflow -- each of these feature subsets will have different optimal hyperparameters!\n",
    "For instance, imagine the extreme cases where we prune all but one feature.\n",
    "Of course the optimal $k$ could be different here than in the case where we include all the features.\n",
    "Likewise with distance weighting.\n",
    "To be sure you have the best possible model, you need to optimize features and hyperparameters together.\n",
    "This often requires training thousands of models!\n",
    "\n",
    "For this reason, there is always a balance between practicality and performance.\n",
    "If you will spend hundreds or thousands of compute hours and gain only 1% improved performance you are probably wasting your time and resources.\n",
    "As a result, it's best to start small and work your way up in complexity if you are seeing improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d88eb",
   "metadata": {},
   "source": [
    "## [Check your understanding]\n",
    "\n",
    "(a) Implement cross-fold validation inside the GA fitness function and use the mean validation $R^2$ as the feature fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d6950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90b85de4",
   "metadata": {},
   "source": [
    "(b) Alternatively, add an option to perform hyperparameter tuning for the `n_neighbors` and `weighting` of `KNeighborsRegressor` (together with feature selection).\n",
    "> You will need to split apart your `features` array into different parts when you get inside the `fitness` function. For instance,\n",
    "```\n",
    "k = features[0]\n",
    "w = features[1]\n",
    "sel = features[2:]\n",
    "```\n",
    "> You may also need to manipulate the values such as rounding to `int`, mapping to discrete options for `str`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015b0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63438ef4",
   "metadata": {},
   "source": [
    "(c) Alternatively, add an option to select both which features will be used and which model.\n",
    "> You will need to map an integer index to a model definition. One way is to use a predefined list like so:\n",
    "\n",
    "```\n",
    "# before the fitness function:\n",
    "possible_models = [linear_model.LinearRegression(), ensemble.RandomForestRegressor(), neighbors.KNeighborsRegressor(), ...]\n",
    "\n",
    "# then inside the fitness function:\n",
    "model = possible_models[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f38c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-id,-colab,-outputId",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
